import torch
import sys
import os

def main():
    # æ£€æŸ¥å‚æ•°
    if len(sys.argv) < 3:
        print("Usage: python tools/strip_head.py <in_checkpoint> <out_checkpoint>")
        print("Example: python tools/strip_head.py work_dirs/phase2/epoch_12.pth checkpoints/epoch_12_headless.pth")
        return

    in_path = sys.argv[1]
    out_path = sys.argv[2]

    # 1. æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(in_path):
        print(f"Error: Input checkpoint '{in_path}' does not exist!")
        return

    print(f"ğŸ” Loading checkpoint from: {in_path}")
    try:
        checkpoint = torch.load(in_path, map_location='cpu')
    except Exception as e:
        print(f"Error loading checkpoint: {e}")
        return
    
    # 2. è·å– state_dict (å…¼å®¹ä¸åŒæ ¼å¼)
    if 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
        meta_info = checkpoint.get('meta', {})
    else:
        state_dict = checkpoint
        meta_info = {}
        
    new_state_dict = {}
    deleted_keys = []
    
    # 3. æ ¸å¿ƒé€»è¾‘ï¼šåˆ é™¤æ£€æµ‹å¤´æƒé‡
    print("âœ‚ï¸  Stripping 'bbox_head' weights...")
    for k, v in state_dict.items():
        # è¿™é‡ŒåŒ¹é… 'bbox_head'ï¼Œè¿™æ˜¯ MMDetection3D ä¸­æ£€æµ‹å¤´çš„æ ‡å‡†å‘½åå‰ç¼€
        if 'bbox_head' in k:
            deleted_keys.append(k)
        else:
            new_state_dict[k] = v
            
    # 4. é‡æ–°å°è£…
    if 'state_dict' in checkpoint:
        checkpoint['state_dict'] = new_state_dict
    else:
        checkpoint = new_state_dict
        
    # 5. ä¿å­˜
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    out_dir = os.path.dirname(out_path)
    if out_dir and not os.path.exists(out_dir):
        os.makedirs(out_dir)

    torch.save(checkpoint, out_path)
    
    print("-" * 50)
    print(f"âœ… Success! Removed {len(deleted_keys)} keys related to detection head.")
    print(f"ğŸ’¾ Headless checkpoint saved to: {out_path}")
    print("-" * 50)

if __name__ == '__main__':
    main()